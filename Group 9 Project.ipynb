{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Group Project 9**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Members\n",
    "## Duygu 8815048\n",
    "## Varun 8894799\n",
    "## Rohit 8895806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting data from Reddit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "# Reddit API credentials\n",
    "CLIENT_ID = \"QhxM4QaIZxn2yIxdOByVOA\"\n",
    "CLIENT_SECRET = \"J0eBuxj7AM5yev0qzlaxZf9O3Mr9ZA\"\n",
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=True,\n",
    ")\n",
    "def get_posts(subreddit, count=10):\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "    posts = subreddit.hot(limit=count)\n",
    "    post_data = {\n",
    "        \"Title\": [],\n",
    "        \"Author\": [],\n",
    "        \"Score\": [],\n",
    "        \"Full Text\": [],\n",
    "        \"URL\": [],\n",
    "    }\n",
    "    for post in posts:\n",
    "        post_data[\"Title\"].append(post.title)\n",
    "        post_data[\"Author\"].append(str(post.author))\n",
    "        post_data[\"Score\"].append(post.score)\n",
    "        post_data[\"Full Text\"].append(post.selftext)\n",
    "        post_data[\"URL\"].append(post.url)\n",
    "\n",
    "    df = pd.DataFrame(post_data)\n",
    "    return df\n",
    "\n",
    "post_count = 1000  # Number of posts to fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting data with respect to data topics**\n",
    "### US POLITICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trump = get_posts('trumptweets', post_count)\n",
    "df_trump.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uspolitics = get_posts('uspolitics', post_count)\n",
    "uspolitics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamalaharris = get_posts(\"kamalaharris\", post_count)\n",
    "\n",
    "joebiden = get_posts(\"joebiden\", post_count)\n",
    "\n",
    "democraticparty = get_posts(\"democraticparty\", post_count)\n",
    "\n",
    "billclinton = get_posts(\"billclinton\", post_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting raw text data from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.concat([kamalaharris,joebiden,democraticparty,billclinton,uspolitics,df_trump], ignore_index=True)\n",
    "data.head()\n",
    "data.to_csv('us_elections.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face pre-trained model for ground truth labelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Assignments\\Fundamental of AAIML\\Assignemnt week 2\\CSCN8010\\venv\\tensorflow_cpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Extracting the Title column from the data\n",
    "texts = data['Title'].tolist()\n",
    "\n",
    "# Load the sentiment analysis pipeline with the BERTweet model\n",
    "model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "# Use the sentiment analysis pipeline to label the text data\n",
    "results = sentiment_analyzer(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping of Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m label_mapping \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPostive\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mNEU\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mNEG\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m }  \n\u001b[1;32m----> 7\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39msentiment_label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [label_mapping[result[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results]\n\u001b[0;32m      8\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mconfidence_score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [result[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    'POS': \"Postive\",\n",
    "    'NEU': \"Neutral\",\n",
    "    'NEG': \"Negative\"\n",
    "}  \n",
    "\n",
    "data['sentiment_label'] = [label_mapping[result['label']] for result in results]\n",
    "data['confidence_score'] = [result['score'] for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheking for class balance in the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "Neutral     2208\n",
       "Negative    1108\n",
       "Postive      373\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting mapped labelled data to csv for processing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('us_polotics_sentiment_confidence.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emojis collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_with_feelings = [\n",
    "    (\"ðŸ˜Š\", \"happy\"),\n",
    "    (\"â¤ï¸\", \"love\"),\n",
    "    (\"ðŸ˜¢\", \"sad\"),\n",
    "    (\"ðŸ˜‚\", \"laughing\"),\n",
    "    (\"ðŸ˜\", \"admiration\"),\n",
    "    (\"ðŸ˜Ž\", \"cool\"),\n",
    "    (\"ðŸ˜­\", \"crying\"),\n",
    "    (\"ðŸ˜¡\", \"angry\"),\n",
    "    (\"ðŸ˜´\", \"sleepy\"),\n",
    "    (\"ðŸ¤”\", \"confused\"),\n",
    "    (\"ðŸ˜Œ\", \"relieved\"),\n",
    "    (\"ðŸ˜‹\", \"delicious\"),\n",
    "    (\"ðŸ˜’\", \"unamused\"),\n",
    "    (\"ðŸ¤—\", \"hugging\"),\n",
    "    (\"ðŸ˜Ÿ\", \"worried\"),\n",
    "    (\"ðŸ˜‡\", \"angelic\"),\n",
    "    (\"ðŸ˜–\", \"frustrated\"),\n",
    "    (\"ðŸ™„\", \"annoyed\"),\n",
    "    (\"ðŸ˜¨\", \"scared\"),\n",
    "    (\"ðŸ¥°\", \"loving\"),\n",
    "    (\"ðŸ¥º\", \"pleading\"),\n",
    "    (\"ðŸ˜µ\", \"dizzy\"),\n",
    "    (\"ðŸ˜ˆ\", \"mischievous\"),\n",
    "    (\"ðŸ˜³\", \"embarrassed\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated\"),\n",
    "    (\"ðŸ˜‘\", \"expressionless\"),\n",
    "    (\"ðŸ˜“\", \"anxious\"),\n",
    "    (\"ðŸ˜”\", \"pensive\"),\n",
    "    (\"ðŸ¥³\", \"celebrating\"),\n",
    "    (\"ðŸ˜¬\", \"grimacing\"),\n",
    "    (\"ðŸ˜\", \"neutral\"),\n",
    "    (\"ðŸ˜•\", \"confused\"),\n",
    "    (\"ðŸ˜ž\", \"disappointed\"),\n",
    "    (\"ðŸ˜²\", \"astonished\"),\n",
    "    (\"ðŸ¤«\", \"shushing\"),\n",
    "    (\"ðŸ˜ª\", \"sleepy\"),\n",
    "    (\"ðŸ¤¥\", \"lying\"),\n",
    "    (\"ðŸ˜–\", \"conflicted\"),\n",
    "    (\"ðŸ¤¡\", \"clownish\"),\n",
    "    (\"ðŸ˜­\", \"sobbing\"),\n",
    "    (\"ðŸ¤§\", \"sneezing\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ™ƒ\", \"upside-down\"),\n",
    "    (\"ðŸ¤¯\", \"mind-blown\"),\n",
    "    (\"ðŸ¥±\", \"yawning\"),\n",
    "    (\"ðŸ˜¤\", \"exasperated\"),\n",
    "    (\"ðŸ˜®â€ðŸ’¨\", \"mind-blown\"),\n",
    "    (\"ðŸ¥²\", \"teary-eyed\"),\n",
    "    (\"ðŸ¥µ\", \"hot\"),\n",
    "    (\"ðŸ¥¶\", \"cold\"),\n",
    "    (\"ðŸ˜´\", \"sleepy\"),\n",
    "    (\"ðŸ¤”\", \"pondering\"),\n",
    "    (\"ðŸ¥´\", \"woozy\"),\n",
    "    (\"ðŸ˜¬\", \"grimacing\"),\n",
    "    (\"ðŸ¤¢\", \"nauseous\"),\n",
    "    (\"ðŸ¤\", \"zipped lips\"),\n",
    "    (\"ðŸ¤•\", \"hurt\"),\n",
    "    (\"ðŸ¥³\", \"celebrating\"),\n",
    "    (\"ðŸ˜ˆ\", \"mischievous\"),\n",
    "    (\"ðŸ˜–\", \"frustrated\"),\n",
    "    (\"ðŸ¤ª\", \"crazy\"),\n",
    "    (\"ðŸ¤¤\", \"drooling\"),\n",
    "    (\"ðŸ¤®\", \"vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"cursing\"),\n",
    "    (\"ðŸ¥º\", \"begging\"),\n",
    "    (\"ðŸ¥±\", \"yawning\"),\n",
    "    (\"ðŸ¥³\", \"partying\"),\n",
    "    (\"ðŸ¥°\", \"blissful\"),\n",
    "    (\"ðŸ¥µ\", \"overheated\"),\n",
    "    (\"ðŸ¥¶\", \"freezing\"),\n",
    "    (\"ðŸ¤ \", \"cowboy\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth\"),\n",
    "    (\"ðŸ¤“\", \"nerdy\"),\n",
    "    (\"ðŸ¤—\", \"hugging\"),\n",
    "    (\"ðŸ¤¡\", \"clown\"),\n",
    "    (\"ðŸ¤¥\", \"liar\"),\n",
    "    (\"ðŸ¤«\", \"shushing\"),\n",
    "    (\"ðŸ¤¬\", \"angry\"),\n",
    "    (\"ðŸ¤¯\", \"mind-blown\"),\n",
    "    (\"ðŸ§\", \"inquiring\"),\n",
    "    (\"ðŸ¤ª\", \"zany\"),\n",
    "    (\"ðŸ¤©\", \"excited\"),\n",
    "    (\"ðŸ¤”\", \"thoughtful\"),\n",
    "    (\"ðŸ¤¨\", \"skeptical\"),\n",
    "    (\"ðŸ¤«\", \"quiet\"),\n",
    "    (\"ðŸ§\", \"curious\"),\n",
    "    (\"ðŸ¤ª\", \"wacky\"),\n",
    "    (\"ðŸ¤©\", \"wonderful\"),\n",
    "    (\"ðŸ¤—\", \"friendly\"),\n",
    "    (\"ðŸ¤¬\", \"furious\"),\n",
    "    (\"ðŸ¤¯\", \"astounding\"),\n",
    "    (\"ðŸ§\", \"inspecting\"),\n",
    "    (\"ðŸ¤“\", \"bookish\"),\n",
    "    (\"ðŸ¤¨\", \"questioning\"),\n",
    "    (\"ðŸ¤«\", \"silent\"),\n",
    "    (\"ðŸ§\", \"analyzing\"),\n",
    "    (\"ðŸ¤ª\", \"crazy\"),\n",
    "    (\"ðŸ¤©\", \"fantastic\"),\n",
    "    (\"ðŸ¤”\", \"pondering\"),\n",
    "    (\"ðŸ¤¨\", \"doubting\"),\n",
    "    (\"ðŸ¤«\", \"hushed\"),\n",
    "    (\"ðŸ¤\", \"mute\"),\n",
    "    (\"ðŸ¤¨\", \"inquisitive\"),\n",
    "    (\"ðŸ¤“\", \"geeky\"),\n",
    "    (\"ðŸ¤—\", \"hug\"),\n",
    "    (\"ðŸ¤–\", \"robotic\"),\n",
    "    (\"ðŸ¤§\", \"sneezing\"),\n",
    "    (\"ðŸ¤£\", \"ROFL\"),\n",
    "    (\"ðŸ¤¡\", \"clown\"),\n",
    "    (\"ðŸ¤¢\", \"sick\"),\n",
    "    (\"ðŸ¤\", \"zip-it\"),\n",
    "    (\"ðŸ¤•\", \"ouch\"),\n",
    "    (\"ðŸ¥³\", \"party\"),\n",
    "    (\"ðŸ˜ˆ\", \"devilish\"),\n",
    "    (\"ðŸ˜–\", \"upset\"),\n",
    "    (\"ðŸ¤ª\", \"silly\"),\n",
    "    (\"ðŸ¤¤\", \"hungry\"),\n",
    "    (\"ðŸ¤®\", \"disgust\"),\n",
    "    (\"ðŸ¤©\", \"amazed\"),\n",
    "    (\"ðŸ¤¬\", \"angry\"),\n",
    "    (\"ðŸ¥º\", \"pleading\"),\n",
    "    (\"ðŸ¥±\", \"tired\"),\n",
    "    (\"ðŸ¥³\", \"celebration\"),\n",
    "    (\"ðŸ¥°\", \"adoration\"),\n",
    "    (\"ðŸ¥µ\", \"hot\"),\n",
    "    (\"ðŸ¥¶\", \"cold\"),\n",
    "    (\"ðŸ¤ \", \"cowboy\"),\n",
    "    (\"ðŸ¤‘\", \"money\"),\n",
    "    (\"ðŸ¤“\", \"nerd\"),\n",
    "    (\"ðŸ¤—\", \"warm\"),\n",
    "    (\"ðŸ¤¡\", \"fool\"),\n",
    "    (\"ðŸ¤¥\", \"lie\"),\n",
    "    (\"ðŸ¤«\", \"shh\"),\n",
    "    (\"ðŸ¤¬\", \"rage\"),\n",
    "    (\"ðŸ¤¯\", \"explode\"),\n",
    "    (\"ðŸ§\", \"detective\"),\n",
    "    (\"ðŸ¤ª\", \"goofy\"),\n",
    "    (\"ðŸ¤©\", \"excite\"),\n",
    "    (\"ðŸ¤”\", \"think\"),\n",
    "    (\"ðŸ¤¨\", \"suspicious\"),\n",
    "    (\"ðŸ¤«\", \"quiet\"),\n",
    "    (\"ðŸ§\", \"observe\"),\n",
    "    (\"ðŸ¤ª\", \"crazy\"),\n",
    "    (\"ðŸ¤©\", \"amazing\"),\n",
    "    (\"ðŸ¤—\", \"embrace\"),\n",
    "    (\"ðŸ¤¬\", \"angry\"),\n",
    "    (\"ðŸ¤¯\", \"unbelievable\"),\n",
    "    (\"ðŸ§\", \"inspect\"),\n",
    "    (\"ðŸ¤“\", \"knowledge\"),\n",
    "    (\"ðŸ¤¨\", \"doubt\"),\n",
    "    (\"ðŸ¤«\", \"silent\"),\n",
    "    (\"ðŸ§\", \"examine\"),\n",
    "    (\"ðŸ¤ª\", \"wacky\"),\n",
    "    (\"ðŸ¤©\", \"awesome\"),\n",
    "    (\"ðŸ¤”\", \"ponder\"),\n",
    "    (\"ðŸ¤¨\", \"mistrust\"),\n",
    "    (\"ðŸ¤«\", \"hush\"),\n",
    "    (\"ðŸ¤\", \"mute\"),\n",
    "    (\"ðŸ¤¨\", \"curious\"),\n",
    "    (\"ðŸ¤“\", \"studious\"),\n",
    "    (\"ðŸ¤—\", \"hug\"),\n",
    "    (\"ðŸ¤–\", \"robot\"),\n",
    "    (\"ðŸ¤§\", \"sneeze\"),\n",
    "    (\"ðŸ¤£\", \"laugh\"),\n",
    "    (\"ðŸ¤¡\", \"jester\"),\n",
    "    (\"ðŸ¤¢\", \"nausea\"),\n",
    "    (\"ðŸ¤\", \"silence\"),\n",
    "    (\"ðŸ¤•\", \"injured\"),\n",
    "    (\"ðŸ¥³\", \"festive\"),\n",
    "    (\"ðŸ˜ˆ\", \"devil\"),\n",
    "    (\"ðŸ˜–\", \"distressed\"),\n",
    "    (\"ðŸ¤ª\", \"eccentric\"),\n",
    "    (\"ðŸ¤¤\", \"mouth-watering\"),\n",
    "    (\"ðŸ¤®\", \"disgusted\"),\n",
    "    (\"ðŸ¤©\", \"awe\"),\n",
    "    (\"ðŸ¤¬\", \"outraged\"),\n",
    "    (\"ðŸ¥º\", \"plead\"),\n",
    "    (\"ðŸ¥±\", \"yawn\"),\n",
    "    (\"ðŸ¥³\", \"celebrate\"),\n",
    "    (\"ðŸ¥°\", \"bliss\"),\n",
    "    (\"ðŸ¥µ\", \"overheated\"),\n",
    "    (\"ðŸ¥¶\", \"freezing\"),\n",
    "    (\"ðŸ¤ \", \"wild west\"),\n",
    "    (\"ðŸ¤‘\", \"wealth\"),\n",
    "    (\"ðŸ¤“\", \"intellectual\"),\n",
    "    (\"ðŸ¤—\", \"affection\"),\n",
    "    (\"ðŸ¤¡\", \"comical\"),\n",
    "    (\"ðŸ¤¥\", \"dishonest\"),\n",
    "    (\"ðŸ¤«\", \"quietness\"),\n",
    "    (\"ðŸ¤¬\", \"wrath\"),\n",
    "    (\"ðŸ¤¯\", \"mind blown\"),\n",
    "    (\"ðŸ§\", \"scrutinize\"),\n",
    "    (\"ðŸ¤ª\", \"offbeat\"),\n",
    "    (\"ðŸ¤©\", \"fantastical\"),\n",
    "    (\"ðŸ¤”\", \"reflective\"),\n",
    "    (\"ðŸ¤¨\", \"distrustful\"),\n",
    "    (\"ðŸ¤«\", \"hushed\"),\n",
    "    (\"ðŸ§\", \"inquisitive\"),\n",
    "    (\"ðŸ¤ª\", \"madcap\"),\n",
    "    (\"ðŸ¤©\", \"awe-inspiring\"),\n",
    "    (\"ðŸ¤—\", \"cordial\"),\n",
    "    (\"ðŸ¤¬\", \"fury\"),\n",
    "    (\"ðŸ¤¯\", \"astounded\"),\n",
    "    (\"ðŸ§\", \"inspect\"),\n",
    "    (\"ðŸ¤“\", \"studious\"),\n",
    "    (\"ðŸ¤¨\", \"disbelief\"),\n",
    "    (\"ðŸ¤«\", \"quietude\"),\n",
    "    (\"ðŸ§\", \"scrutinize\"),\n",
    "    (\"ðŸ¤ª\", \"zany\"),\n",
    "    (\"ðŸ¤©\", \"marvelous\"),\n",
    "    (\"ðŸ¤”\", \"thoughtful\"),\n",
    "    (\"ðŸ¤¨\", \"dubious\"),\n",
    "    (\"ðŸ¤«\", \"hush\"),\n",
    "    (\"ðŸ¤\", \"sealed lips\"),\n",
    "    (\"ðŸ¤¨\", \"inquisitive\"),\n",
    "    (\"ðŸ¤“\", \"intellect\"),\n",
    "    (\"ðŸ¤—\", \"warm embrace\"),\n",
    "    (\"ðŸ¤–\", \"robot\"),\n",
    "    (\"ðŸ¤§\", \"achoo\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"green face\"),\n",
    "    (\"ðŸ¤\", \"zipped mouth\"),\n",
    "    (\"ðŸ¤•\", \"bandaged head\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤¬\", \"face with symbols on mouth\"),\n",
    "    (\"ðŸ¥º\", \"pleading face\"),\n",
    "    (\"ðŸ¥±\", \"yawning face\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ¥°\", \"smiling face with hearts\"),\n",
    "    (\"ðŸ¥µ\", \"hot face\"),\n",
    "    (\"ðŸ¥¶\", \"cold face\"),\n",
    "    (\"ðŸ¤ \", \"cowboy hat face\"),\n",
    "    (\"ðŸ¤‘\", \"money-mouth face\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¥\", \"lying face\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤¬\", \"angry face\"),\n",
    "    (\"ðŸ¤¯\", \"exploding head\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ§\", \"face with monocle\"),\n",
    "    (\"ðŸ¤ª\", \"crazy face\"),\n",
    "    (\"ðŸ¤©\", \"star-struck\"),\n",
    "    (\"ðŸ¤”\", \"thinking face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤«\", \"shushing face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤¨\", \"face with raised eyebrow\"),\n",
    "    (\"ðŸ¤“\", \"nerd face\"),\n",
    "    (\"ðŸ¤—\", \"hugging face\"),\n",
    "    (\"ðŸ¤–\", \"robot face\"),\n",
    "    (\"ðŸ¤§\", \"sneezing face\"),\n",
    "    (\"ðŸ¤£\", \"rolling on the floor laughing\"),\n",
    "    (\"ðŸ¤¡\", \"clown face\"),\n",
    "    (\"ðŸ¤¢\", \"nauseated face\"),\n",
    "    (\"ðŸ¤\", \"zipper-mouth face\"),\n",
    "    (\"ðŸ¤•\", \"face with head-bandage\"),\n",
    "    (\"ðŸ¥³\", \"partying face\"),\n",
    "    (\"ðŸ˜ˆ\", \"smiling face with horns\"),\n",
    "    (\"ðŸ˜–\", \"confounded face\"),\n",
    "    (\"ðŸ¤ª\", \"zany face\"),\n",
    "    (\"ðŸ¤¤\", \"drooling face\"),\n",
    "    (\"ðŸ¤®\", \"face vomiting\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install slang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing function for emojis handling, slang handling, punctuation, stop-words, hashtags and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text emojis happy sad joyful laugh loud way oh god\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Custom slang dictionary\n",
    "slang_dict = {\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"b4\": \"before\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"ur\": \"your\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"pls\": \"please\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"g2g\": \"got to go\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"otp\": \"on the phone\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"tgif\": \"thank goodness it's Friday\",\n",
    "    \"wfh\": \"work from home\",\n",
    "}\n",
    "\n",
    "\n",
    "def handle_emojis_with_lemmatizer1(text):\n",
    "    # Converting emojis to their emotional descriptions\n",
    "    for emoji_char, emoji_description in emojis_with_feelings:\n",
    "        text = text.replace(emoji_char, \" \" + emoji_description)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove hashtags using regex\n",
    "    pattern = r'#\\w+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Handle slang words using the custom slang dictionary\n",
    "    words = word_tokenize(text)\n",
    "    words_with_slang_replaced = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "    text = \" \".join(words_with_slang_replaced)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words_without_stopwords = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    blob = TextBlob(\" \".join(words_without_stopwords))\n",
    "    lemmatized_words = [word.lemmatize() for word in blob.words]\n",
    "\n",
    "    # Reconstructing the text with lemmatized words\n",
    "    lemmatized_text_with_emojis = \" \".join(lemmatized_words)\n",
    "    for emoji_description, emoji_char in emojis_with_feelings:\n",
    "        lemmatized_text_with_emojis = lemmatized_text_with_emojis.replace(emoji_description, emoji_char)\n",
    "\n",
    "    return lemmatized_text_with_emojis\n",
    "\n",
    "# Sa,ple test case\n",
    "emojis_with_feelings = [(\"ðŸ˜Š\", \"happy\"), (\"ðŸ˜¢\", \"sad\"), (\"ðŸ˜„\", \"joyful\")]\n",
    "text = \"This is a sample text with #hashtag and emojis ðŸ˜ŠðŸ˜¢ðŸ˜„. lol btw omg\"\n",
    "\n",
    "preprocessed_text = handle_emojis_with_lemmatizer1(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the labelled dataset and applying the above created preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_new = pd.read_csv('us_polotics_sentiment_confidence.csv')\n",
    "\n",
    "df_new['preprocessed_text'] = df_new['Title'].apply(handle_emojis_with_lemmatizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: gensim in d:\\assignments\\fundamental of aaiml\\assignemnt week 2\\cscn8010\\venv\\tensorflow_cpu\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\assignments\\fundamental of aaiml\\assignemnt week 2\\cscn8010\\venv\\tensorflow_cpu\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\assignments\\fundamental of aaiml\\assignemnt week 2\\cscn8010\\venv\\tensorflow_cpu\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\assignments\\fundamental of aaiml\\assignemnt week 2\\cscn8010\\venv\\tensorflow_cpu\\lib\\site-packages (from gensim) (6.3.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "preprocessed_texts = df_new['preprocessed_text'].tolist()\n",
    "\n",
    "sentence_tokens = [sent_tokenize(text) for text in preprocessed_texts]\n",
    "\n",
    "all_words = [word for tokens in sentence_tokens for word in word_tokenize(\" \".join(tokens))]\n",
    "\n",
    "# Load or train a Word2Vec model on your preprocessed text\n",
    "word2vec_model = Word2Vec(sentences=[all_words], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "sentence_vectors = []\n",
    "\n",
    "for tokens in sentence_tokens:\n",
    "    word_vectors = [word2vec_model.wv[word] for word in word_tokenize(\" \".join(tokens)) if word in word2vec_model.wv]\n",
    "\n",
    "    if len(word_vectors) > 0:\n",
    "        sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        sentence_vector = np.zeros(word2vec_model.vector_size)\n",
    "    sentence_vectors.append(sentence_vector)\n",
    "\n",
    "sentence_vectors_array = np.array(sentence_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_2_vec = sentence_vectors_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe consisting word 2 Vec representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_2_vec = pd.DataFrame(X_word_2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the dependent feature 'sentiments' to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_2_vec['sentiments'] = df_new['sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000418</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>0.011896</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>-0.010930</td>\n",
       "      <td>0.023879</td>\n",
       "      <td>-0.014073</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.003721</td>\n",
       "      <td>0.013060</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003194</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>-0.011787</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>-0.007830</td>\n",
       "      <td>0.019383</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>-0.003774</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003045</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>-0.010769</td>\n",
       "      <td>-0.006018</td>\n",
       "      <td>-0.007702</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>-0.007238</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.021969</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>-0.008873</td>\n",
       "      <td>-0.010274</td>\n",
       "      <td>-0.009320</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>-0.011471</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.002028</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>-0.004554</td>\n",
       "      <td>-0.006702</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>-0.007935</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>-0.001930</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.010147</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>-0.000588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>-0.001997</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>-0.003426</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>-0.002695</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000588</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>-0.003247</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.001291</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>-0.000583</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>-0.001831</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>-0.002248</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>-0.003925</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>0.008512</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>-0.002200</td>\n",
       "      <td>-0.002406</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>-0.003228</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>-0.003075</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>-0.008764</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>-0.001081</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3689 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.000418  0.027125  0.021528  0.011896 -0.011664 -0.010974 -0.010930   \n",
       "1    -0.003194  0.024958  0.019977  0.012267 -0.011787 -0.009225 -0.007830   \n",
       "2    -0.003045  0.018391  0.017880  0.009451 -0.010769 -0.006018 -0.007702   \n",
       "3    -0.001312  0.021969  0.018271  0.008554 -0.008873 -0.010274 -0.009320   \n",
       "4    -0.000141  0.016063  0.014356  0.007500 -0.004554 -0.006702 -0.005220   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3684 -0.000656  0.003123  0.002424  0.002323  0.000265  0.001864  0.000564   \n",
       "3685  0.001347  0.005754  0.002770  0.002099  0.001233 -0.000292  0.001007   \n",
       "3686 -0.003247  0.000725 -0.001291  0.001829  0.000977  0.001396  0.001948   \n",
       "3687 -0.003925  0.000800 -0.000181 -0.000889 -0.002131  0.000429 -0.004807   \n",
       "3688 -0.003075  0.004307  0.008344  0.007030 -0.002150 -0.001391  0.000372   \n",
       "\n",
       "             7         8         9  ...        91        92        93  \\\n",
       "0     0.023879 -0.014073  0.002420  ...  0.007013 -0.001046 -0.003721   \n",
       "1     0.019383 -0.012400  0.005479  ...  0.005322  0.002225 -0.003774   \n",
       "2     0.015567 -0.007238  0.005486  ...  0.003244  0.004433 -0.005191   \n",
       "3     0.025276 -0.011471  0.002000  ...  0.004785  0.000035 -0.002028   \n",
       "4     0.013989 -0.007935  0.000824  ...  0.002898  0.001020 -0.001930   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3684  0.000965 -0.001029 -0.000588  ...  0.002271  0.002686 -0.000179   \n",
       "3685  0.003601 -0.003426  0.000524  ...  0.001175 -0.002695  0.001612   \n",
       "3686  0.004477 -0.000583 -0.000388  ...  0.000908  0.001590 -0.001831   \n",
       "3687  0.008512 -0.005782  0.002004  ...  0.003138 -0.002200 -0.002406   \n",
       "3688  0.003044 -0.008764  0.006142  ...  0.005198  0.002486 -0.001081   \n",
       "\n",
       "            94        95        96        97        98        99  sentiments  \n",
       "0     0.013060  0.016309  0.004236  0.003606  0.006064  0.012041     Neutral  \n",
       "1     0.012716  0.014896  0.005427  0.004365  0.003829  0.012512     Neutral  \n",
       "2     0.007448  0.009034  0.003046  0.004493  0.001803  0.010924     Neutral  \n",
       "3     0.012751  0.014262  0.002001  0.005669  0.003250  0.014472     Neutral  \n",
       "4     0.007229  0.008826  0.003746  0.004330  0.003308  0.010147     Neutral  \n",
       "...        ...       ...       ...       ...       ...       ...         ...  \n",
       "3684  0.000351  0.002269 -0.001997 -0.000379  0.002756 -0.000485     Neutral  \n",
       "3685  0.002959  0.002007 -0.000447 -0.000588  0.003323  0.002807     Neutral  \n",
       "3686  0.004391 -0.002248  0.002438 -0.001717  0.001066  0.002862     Neutral  \n",
       "3687  0.003787  0.008033 -0.003228 -0.000342  0.003000  0.003768     Neutral  \n",
       "3688  0.006348  0.001238  0.003542 -0.000075  0.004691  0.003710     Neutral  \n",
       "\n",
       "[3689 rows x 101 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_2_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000418</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>0.011896</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>-0.010930</td>\n",
       "      <td>0.023879</td>\n",
       "      <td>-0.014073</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.003721</td>\n",
       "      <td>0.013060</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003194</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>-0.011787</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>-0.007830</td>\n",
       "      <td>0.019383</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>-0.003774</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003045</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>-0.010769</td>\n",
       "      <td>-0.006018</td>\n",
       "      <td>-0.007702</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>-0.007238</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.021969</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>-0.008873</td>\n",
       "      <td>-0.010274</td>\n",
       "      <td>-0.009320</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>-0.011471</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.002028</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>-0.004554</td>\n",
       "      <td>-0.006702</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>-0.007935</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>-0.001930</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.010147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.000418  0.027125  0.021528  0.011896 -0.011664 -0.010974 -0.010930   \n",
       "1 -0.003194  0.024958  0.019977  0.012267 -0.011787 -0.009225 -0.007830   \n",
       "2 -0.003045  0.018391  0.017880  0.009451 -0.010769 -0.006018 -0.007702   \n",
       "3 -0.001312  0.021969  0.018271  0.008554 -0.008873 -0.010274 -0.009320   \n",
       "4 -0.000141  0.016063  0.014356  0.007500 -0.004554 -0.006702 -0.005220   \n",
       "\n",
       "          7         8         9  ...        91        92        93        94  \\\n",
       "0  0.023879 -0.014073  0.002420  ...  0.007013 -0.001046 -0.003721  0.013060   \n",
       "1  0.019383 -0.012400  0.005479  ...  0.005322  0.002225 -0.003774  0.012716   \n",
       "2  0.015567 -0.007238  0.005486  ...  0.003244  0.004433 -0.005191  0.007448   \n",
       "3  0.025276 -0.011471  0.002000  ...  0.004785  0.000035 -0.002028  0.012751   \n",
       "4  0.013989 -0.007935  0.000824  ...  0.002898  0.001020 -0.001930  0.007229   \n",
       "\n",
       "         95        96        97        98        99  sentiments  \n",
       "0  0.016309  0.004236  0.003606  0.006064  0.012041           1  \n",
       "1  0.014896  0.005427  0.004365  0.003829  0.012512           1  \n",
       "2  0.009034  0.003046  0.004493  0.001803  0.010924           1  \n",
       "3  0.014262  0.002001  0.005669  0.003250  0.014472           1  \n",
       "4  0.008826  0.003746  0.004330  0.003308  0.010147           1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df_word_2_vec[\"sentiments\"] = encoder.fit_transform(df_word_2_vec[\"sentiments\"])\n",
    "df_word_2_vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into dependent and independent set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_2_vec = df_word_2_vec.iloc[:,:-1]\n",
    "y_word_2_vec = df_word_2_vec.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled_w2_v, y_resampled_w2_v = smote.fit_resample(X_word_2_vec, y_word_2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_w2_v, y_resampled_w2_v, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "classifier = rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf1 = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix, Accuracy, Precision and Recall and Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[344  88   1]\n",
      " [ 95 339  26]\n",
      " [  8  10 414]]\n",
      "The accuracy of Random Forest Classifier using word2vec is 0.8279245283018868\n",
      "The precision of Random Forest Classifier using word2vec is 0.8280313871231638\n",
      "The recall of Random Forest Classifier using word2vec is  0.8299157099664178\n",
      "------------------------------------------------------------------\n",
      "[0.79433962 0.83018868 0.78113208 0.81320755 0.81132075 0.81509434\n",
      " 0.81886792 0.82075472 0.85471698 0.80907372]\n",
      "-------------------------------------------------------------\n",
      "The mean cross validation accuracy is 0.8148696365516997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cm_rf = confusion_matrix(y_test,y_pred_rf1)\n",
    "print(cm_rf)\n",
    "accuracy_score_rf1 = accuracy_score(y_test,y_pred_rf1)\n",
    "\n",
    "acc_rf = accuracy_score(y_test,y_pred_rf1)\n",
    "print(\"The accuracy of Random Forest Classifier using word2vec is\", acc_rf)\n",
    "\n",
    "precision_rf = precision_score(y_test, y_pred_rf1, average='macro')\n",
    "print(\"The precision of Random Forest Classifier using word2vec is\",precision_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf1, average='macro')\n",
    "print(\"The recall of Random Forest Classifier using word2vec is \",recall_rf)\n",
    "\n",
    "print(\"------------------------------------------------------------------\")\n",
    "scores_rf1 = cross_val_score(classifier,X_train,y_train,cv = 10, scoring = \"accuracy\")\n",
    "print(scores_rf1)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"The mean cross validation accuracy is\", scores_rf1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X_tfidf = cv.fit_transform(df_new['preprocessed_text']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tf-idf dataframe with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7332</th>\n",
       "      <th>7333</th>\n",
       "      <th>7334</th>\n",
       "      <th>7335</th>\n",
       "      <th>7336</th>\n",
       "      <th>7337</th>\n",
       "      <th>7338</th>\n",
       "      <th>7339</th>\n",
       "      <th>7340</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7342 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  7332  7333  7334  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   7335  7336  7337  7338  7339  7340  sentiments  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0     Neutral  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0     Neutral  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0     Neutral  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0     Neutral  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0     Neutral  \n",
       "\n",
       "[5 rows x 7342 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X_tfidf)\n",
    "df_tfidf['sentiments'] = df_new['sentiment_label']\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7332</th>\n",
       "      <th>7333</th>\n",
       "      <th>7334</th>\n",
       "      <th>7335</th>\n",
       "      <th>7336</th>\n",
       "      <th>7337</th>\n",
       "      <th>7338</th>\n",
       "      <th>7339</th>\n",
       "      <th>7340</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7342 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  7332  7333  7334  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   7335  7336  7337  7338  7339  7340  sentiments  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0           1  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0           1  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0           1  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0           1  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0           1  \n",
       "\n",
       "[5 rows x 7342 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df_tfidf[\"sentiments\"] = encoder.fit_transform(df_tfidf[\"sentiments\"])\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into independent and dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = df_tfidf.iloc[:,:-1]\n",
    "y_tfidf = df_tfidf.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7331</th>\n",
       "      <th>7332</th>\n",
       "      <th>7333</th>\n",
       "      <th>7334</th>\n",
       "      <th>7335</th>\n",
       "      <th>7336</th>\n",
       "      <th>7337</th>\n",
       "      <th>7338</th>\n",
       "      <th>7339</th>\n",
       "      <th>7340</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3689 rows Ã— 7341 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  7331  \\\n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "3684   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3685   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3686   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3687   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3688   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "      7332  7333  7334  7335  7336  7337  7338  7339  7340  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "3684   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3685   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3686   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3687   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3688   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3689 rows x 7341 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiments\n",
       "2    1776\n",
       "0    1775\n",
       "1    1748\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train the Multinomial Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier_NB = MultinomialNB()\n",
    "classifier_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multinomial = classifier_NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix, Accuracy, Precision and Recall and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[381  32  20]\n",
      " [ 99 269  92]\n",
      " [  3   4 425]]\n",
      "The accuracy of naive bayes classifier using TFIDF is 0.8113207547169812\n",
      "The precision of naive bayes classifier using TFIDF is 0.8207403269612342\n",
      "The recall of naive bayes classifier using TFIDF is  0.8161621754130205\n",
      "[0.81132075 0.78490566 0.81320755 0.78301887 0.80188679 0.81698113\n",
      " 0.78301887 0.83773585 0.81886792 0.80340265]\n",
      "-------------------------------------------------------------\n",
      "The mean cross validation accuracy is 0.805434604272925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score\n",
    "cm_nb_tf = confusion_matrix(y_test,y_pred_multinomial)\n",
    "print(cm_nb_tf)\n",
    "acc_nb_tf = accuracy_score(y_test,y_pred_multinomial)\n",
    "print(\"The accuracy of naive bayes classifier using TFIDF is\", acc_nb_tf)\n",
    "\n",
    "precision_macro_nbtf = precision_score(y_test, y_pred_multinomial, average='macro')\n",
    "print(\"The precision of naive bayes classifier using TFIDF is\",precision_macro_nbtf)\n",
    "\n",
    "recall_macro_nbtf = recall_score(y_test, y_pred_multinomial, average='macro')\n",
    "print(\"The recall of naive bayes classifier using TFIDF is \",recall_macro_nbtf)\n",
    "\n",
    "scores_nbtf = cross_val_score(classifier_NB,X_train,y_train,cv = 10, scoring = \"accuracy\")\n",
    "print(scores_nbtf)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"The mean cross validation accuracy is\", scores_nbtf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TkInter user interface TF-IDF with multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Load the trained Naive Bayes classifier and TF-IDF vectorizer\n",
    "classifier = joblib.load('naive_bayes_classifier.pkl')\n",
    "cv = joblib.load('tfidf_cv.pkl')\n",
    "\n",
    "label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "def handle_emojis_with_lemmatizer1(text):\n",
    "    # Convert emojis to their emotional descriptions\n",
    "    for emoji_char, emoji_description in emojis_with_feelings:\n",
    "        text = text.replace(emoji_char, \" \" + emoji_description)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove hashtags using regex\n",
    "    pattern = r'#\\w+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Handle slang words using the custom slang dictionary\n",
    "    words = word_tokenize(text)\n",
    "    words_with_slang_replaced = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "    text = \" \".join(words_with_slang_replaced)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words_without_stopwords = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    blob = TextBlob(\" \".join(words_without_stopwords))\n",
    "    lemmatized_words = [word.lemmatize() for word in blob.words]\n",
    "\n",
    "    # Reconstruct the text with lemmatized words\n",
    "    lemmatized_text_with_emojis = \" \".join(lemmatized_words)\n",
    "    for emoji_description, emoji_char in emojis_with_feelings:\n",
    "        lemmatized_text_with_emojis = lemmatized_text_with_emojis.replace(emoji_description, emoji_char)\n",
    "\n",
    "    return lemmatized_text_with_emojis\n",
    "\n",
    "def classify_sentence():\n",
    "    sentence = entry.get() \n",
    "    preprocessed_sentence = handle_emojis_with_lemmatizer1(sentence)\n",
    "    sentence_vector = cv.transform([preprocessed_sentence])\n",
    "    predicted_label_encoded = classifier.predict(sentence_vector)[0]\n",
    "\n",
    "    # Get the corresponding text label using the mapping\n",
    "    predicted_label_text = label_mapping[predicted_label_encoded]\n",
    "\n",
    "    result_label.config(text=f'Predicted Label: {predicted_label_text}')\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title('Sentence Classifier')\n",
    "\n",
    "# Create widgets\n",
    "label = tk.Label(root, text='Enter a sentence:')\n",
    "entry = tk.Entry(root, width=40)\n",
    "classify_button = tk.Button(root, text='Classify', command=classify_sentence)\n",
    "result_label = tk.Label(root, text='Predicted Label:')\n",
    "\n",
    "# Layout the widgets\n",
    "label.pack(pady=10)\n",
    "entry.pack(pady=5)\n",
    "classify_button.pack(pady=10)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_bw = cv.fit_transform(df_new['preprocessed_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bw = pd.DataFrame(X_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bw['sentiments'] = df_new['sentiment_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Bag of words dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7332</th>\n",
       "      <th>7333</th>\n",
       "      <th>7334</th>\n",
       "      <th>7335</th>\n",
       "      <th>7336</th>\n",
       "      <th>7337</th>\n",
       "      <th>7338</th>\n",
       "      <th>7339</th>\n",
       "      <th>7340</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7342 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  7332  7333  7334  7335  7336  7337  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "\n",
       "   7338  7339  7340  sentiments  \n",
       "0     0     0     0     Neutral  \n",
       "1     0     0     0     Neutral  \n",
       "2     0     0     0     Neutral  \n",
       "3     0     0     0     Neutral  \n",
       "4     0     0     0     Neutral  \n",
       "\n",
       "[5 rows x 7342 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the dependent Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df_bw[\"sentiments\"] = encoder.fit_transform(df_bw[\"sentiments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into independent and dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bw = df_bw.iloc[:,:-1]\n",
    "y_bw = df_bw.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled_bw, y_resampled_bw = smote.fit_resample(X_bw, y_bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_bw, y_resampled_bw, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB model with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Multinomial Naive Bayes model\n",
    "nb_model_bw = MultinomialNB()\n",
    "nb_model_bw.fit(X_train, y_train)\n",
    "y_pred_bw = nb_model_bw.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix, Accuracy, Precision and Recall and Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  70 174]\n",
      " [101 280  79]\n",
      " [  7  22 403]]\n",
      "The accuracy of naive bayes classifier using bagofwords is 0.6581132075471698\n",
      "The precision of naive bayes classifier using bagofwords is 0.66779369223311\n",
      "The recall of naive bayes classifier using bagofwords is  0.6593518766448612\n",
      "------------------------------------------------------------------------------------\n",
      "[0.6490566  0.57169811 0.61132075 0.58301887 0.56792453 0.59622642\n",
      " 0.62264151 0.68679245 0.65849057 0.61814745]\n",
      "-------------------------------------------------------------\n",
      "The mean cross validation accuracy is 0.6165317259335878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred_bw)\n",
    "print(cm)\n",
    "acc_bw = accuracy_score(y_test,y_pred_bw)\n",
    "print(\"The accuracy of naive bayes classifier using bagofwords is\", acc_bw)\n",
    "\n",
    "precision_bw = precision_score(y_test, y_pred_bw, average='macro')\n",
    "print(\"The precision of naive bayes classifier using bagofwords is\",precision_bw)\n",
    "\n",
    "recall_macro_bw = recall_score(y_test, y_pred_bw, average='macro')\n",
    "\n",
    "print(\"The recall of naive bayes classifier using bagofwords is \",recall_macro_bw)\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "scores_nb_bw = cross_val_score(nb_model_bw,X_train,y_train,cv = 10, scoring = \"accuracy\")\n",
    "print(scores_nb_bw)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"The mean cross validation accuracy is\", scores_nb_bw.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_texts = df_new['preprocessed_text'].tolist()\n",
    "\n",
    "# Tokenize the sentences\n",
    "sentence_tokens = [sent for sent in preprocessed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_new['sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\duygu\\onedrive\\documents\\foundations of machine learning framework\\cscn8010\\venv\\cscn8010_classic_ml\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load pre-trained Glove embeddings\n",
    "glove_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "\n",
    "\n",
    "\n",
    "sentence_vectors = []\n",
    "y_gloved = []\n",
    "\n",
    "index = 0\n",
    "for sentence in sentence_tokens:\n",
    "    tokens = word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    word_vectors = [glove_model[word] for word in tokens if word in glove_model]\n",
    "    if word_vectors:\n",
    "        sentence_vector = np.mean(word_vectors, axis=0)  # Calculate sentence vector by averaging word vectors\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "        y_gloved.append(y[index])\n",
    "        index = index + 1\n",
    "\n",
    "sentence_vectors = np.array(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sentence_vectors, 'glove_sentence_vectors.pkl')\n",
    "joblib.dump(y_gloved, 'labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\duygu\\onedrive\\documents\\foundations of machine learning framework\\cscn8010\\venv\\cscn8010_classic_ml\\lib\\site-packages (from imbalanced-learn->imblearn) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\duygu\\onedrive\\documents\\foundations of machine learning framework\\cscn8010\\venv\\cscn8010_classic_ml\\lib\\site-packages (from imbalanced-learn->imblearn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\duygu\\onedrive\\documents\\foundations of machine learning framework\\cscn8010\\venv\\cscn8010_classic_ml\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\duygu\\onedrive\\documents\\foundations of machine learning framework\\cscn8010\\venv\\cscn8010_classic_ml\\lib\\site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\duygu\\onedrive\\documents\\foundations of machine learning framework\\cscn8010\\venv\\cscn8010_classic_ml\\lib\\site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Using cached imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.11.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(sentence_vectors_array, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_resampled, y_resampled, test_size = 0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and evaluating with Random Forest with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8173584905660377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[337,  90,   6],\n",
       "       [ 99, 337,  24],\n",
       "       [  7,  16, 409]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\",accuracy)\n",
    "\n",
    "cf_mat = confusion_matrix(y_test,y_pred)\n",
    "cf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Precision and Recall and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[337  90   6]\n",
      " [ 99 337  24]\n",
      " [  7  16 409]]\n",
      "The accuracy of Random Fores classifier using glove is 0.8173584905660377\n",
      "The precision of Random Forest classifier using glove is 0.817702521806349\n",
      "The recall of naive bayes classifier using glove is  0.8192196493276755\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe recall of naive bayes classifier using glove is \u001b[39m\u001b[39m\"\u001b[39m,recall_macro_glove)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m------------------------------------------------------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m scores_rf_glv \u001b[39m=\u001b[39m cross_val_score(clf,X_train,y_train,cv \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, scoring \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(scores_rf_glv)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "cm_glove = confusion_matrix(y_test,y_pred)\n",
    "print(cm_glove)\n",
    "acc_glove = accuracy_score(y_test,y_pred)\n",
    "print(\"The accuracy of Random Fores classifier using glove is\", acc_glove)\n",
    "\n",
    "precision_glove = precision_score(y_test, y_pred, average='macro')\n",
    "print(\"The precision of Random Forest classifier using glove is\",precision_glove)\n",
    "\n",
    "recall_macro_glove = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"The recall of naive bayes classifier using glove is \",recall_macro_glove)\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "scores_rf_glv = cross_val_score(clf,X_train,y_train,cv = 10, scoring = \"accuracy\")\n",
    "print(scores_rf_glv)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"The mean cross validation accuracy is\", scores_rf_glv.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove_model.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'random_forest_latest.pkl')\n",
    "joblib.dump(glove_model, 'glove_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface using TkInter for GloVe with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained Random Forest classifier\n",
    "classifier = joblib.load('random_forest_latest.pkl')\n",
    "# Load the pre-trained Glove model from the pickle file\n",
    "glove_model = joblib.load('glove_model.pkl')\n",
    "\n",
    "label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "\n",
    "def classify_sentence():\n",
    "    sentence = entry.get()  # Get the input sentence\n",
    "    \n",
    "    # Preprocess the sentence using the provided function\n",
    "    preprocessed_sentence = handle_emojis_with_lemmatizer1(sentence)\n",
    "    \n",
    "    # Using the loaded Glove model to transform the preprocessed sentence into a vector\n",
    "    tokens = word_tokenize(preprocessed_sentence.lower()) \n",
    "    word_vectors = [glove_model[word] for word in tokens if word in glove_model]\n",
    "    if word_vectors:\n",
    "        sentence_vector = np.mean(word_vectors, axis=0) \n",
    "\n",
    "    # Using the trained Random Forest model to predict the label\n",
    "    predicted_label_encoded = classifier.predict(sentence_vector.reshape(1, -1))\n",
    "    print(predicted_label_encoded[0])\n",
    "    # Get the corresponding text label using the mapping\n",
    "    predicted_label_text = label_mapping[predicted_label_encoded[0]]\n",
    "    \n",
    "    result_label.config(text=f'Predicted Label: {predicted_label_text}')\n",
    "\n",
    "# Creating the main window\n",
    "root = tk.Tk()\n",
    "root.title('Sentence Classifier using GloVe and Random Forest')\n",
    "\n",
    "# Creating widgets\n",
    "label = ttk.Label(root, text='Enter a sentence:')\n",
    "entry = ttk.Entry(root, width=40)\n",
    "classify_button = ttk.Button(root, text='Classify', command=classify_sentence)\n",
    "result_label = ttk.Label(root, text='Predicted Label:')\n",
    "\n",
    "# Layout the widgets\n",
    "label.pack(pady=10)\n",
    "entry.pack(pady=5)\n",
    "classify_button.pack(pady=10)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "# Starting the Tkinter event loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
